<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>CS 184/284A - Homework 1: Rasterizer</title>
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            color: #333;
            max-width: 900px;
            margin: 0 auto;
            padding: 20px;
        }
        
        h1, h2, h3 {
            color: #003262; /* Berkeley Blue */
        }
        
        h1 {
            border-bottom: 2px solid #FDB515; /* Berkeley Gold */
            padding-bottom: 10px;
        }
        
        h2 {
            margin-top: 30px;
            border-bottom: 1px solid #ccc;
            padding-bottom: 5px;
        }
        
        img {
            max-width: 100%;
            height: auto;
            border: 1px solid #ddd;
            margin: 15px 0;
            display: block;
        }
        
        .image-container {
            margin: 20px 0;
        }
        
        .image-caption {
            font-style: italic;
            color: #666;
            margin-top: 5px;
        }
        
        a {
            color: #3B7EA1;
            text-decoration: none;
        }
        
        a:hover {
            text-decoration: underline;
        }
        
        .code {
            font-family: monospace;
            background-color: #f5f5f5;
            padding: 2px 4px;
            border-radius: 3px;
        }
        
        footer {
            margin-top: 50px;
            padding-top: 20px;
            border-top: 1px solid #ddd;
            color: #666;
            font-size: 0.9em;
        }
    </style>
    <!-- MathJax for rendering math expressions -->
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
    <h1>Homework 1: Rasterizer</h1>
    <p><strong>CS 184/284A – Spring 2025</strong></p>
    
    <p><strong>Name:</strong> C.K. Wolfe, Edward Lee</p>
    
    <p><strong>Link to Webpage:</strong> <a href="https://cal-cs184-student.github.io/hw-webpages-frame-drop-fam-1/hw1/index.html">https://cal-cs184-student.github.io/hw-webpages-frame-drop-fam-1/hw1/index.html</a></p>
    
    <p><strong>Link to GitHub Repository:</strong> <a href="https://github.com/redspry/sp25-hw1-X">https://github.com/redspry/sp25-hw1-X</a> (due to GitHub classroom group issues and being unable to switch groups, we had to create an external fork of the GitHub classroom repository)</p>
    
    <h2>Task 1: Drawing Single-Color Triangles</h2>
    
    <p>
        For each triangle, we calculate the bounding box by taking the minimum and maximum x and y coordinates of the triangle's vertices. This ensures only the relevant portion of the screen is tested. For each pixel within that bounding box, we sample at (x + 0.5, y + 0.5) to determine if the center of the pixel lies within the triangle. We use a barycentric coordinate (or half-space) test to check if the sample is inside. If it is, we fill the pixel with the triangle's color using <code>fill_pixel()</code>. Because we only iterate over the bounding box of each triangle, this method is no more expensive than sampling every pixel in the bounding box. In other words, we do not check pixels outside the bounding box, which keeps the algorithm efficient.
    </p>
    
    <p>Below is a PNG screenshot of <strong>basic/test4.svg</strong> with default parameters.</p>
    
    <div class="image-container">
        <img src="./submission-images/screenshot_2-18_23-35-21.png" alt="test4.svg">
    </div>
    
    <p>Here is a screenshot with the pixel inspector zoomed in on the blue triangle.</p>
    
    <div class="image-container">
        <img src="./submission-images/screenshot_2-18_23-37-17.png" alt="test4.svg">
    </div>
    
    <h2>Task 2: Antialiasing by Supersampling</h2>
    
    <p>
        We use a high-resolution buffer to accumulate subpixel color data. When the sample rate is \( n^2 \), each pixel is subdivided into an \( n \times n \) grid of subpixels. In <code>rasterize_triangle()</code>, instead of one center sample per pixel, we now compute subpixel coordinates and test each subpixel for coverage. After rasterization, we average the subpixels in each pixel block to produce the final color in the framebuffer. Supersampling tackles aliasing by capturing partial coverage along edges. Rather than a hard on/off at each pixel center, multiple subpixel samples create smoother transitions by averaging the subpixel colors.
    </p>
    
    <p>
        To add supersampling, we added a new array (<code>sample_buffer</code>) sized <code>(width * height * sample_rate)</code>, updated <code>fill_pixel()</code> to write into the supersample array rather than directly into the framebuffer, and added a <strong>resolve step</strong> (<code>resolve_to_framebuffer()</code>) that downsamples each \( n \times n \) block to one final pixel color.
    </p>
    
    <p>Below are <strong>basic/test4.svg</strong> screenshots at sample rates 1 (no supersampling), 4, and 16. Notice how the edges become smoother at higher sampling rates due to the subpixel averaging.</p>
    
    <div class="image-container">
        <img src="./submission-images/screenshot_2-18_23-48-26.png" alt="test4.svg">
        <div class="image-caption">1 sample per pixel (1 spp)</div>
    </div>
    
    <div class="image-container">
        <img src="./submission-images/screenshot_2-18_23-48-32.png" alt="test4.svg">
        <div class="image-caption">4 samples per pixel (4 spp)</div>
    </div>
    
    <div class="image-container">
        <img src="./submission-images/screenshot_2-18_23-48-45.png" alt="test4.svg">
        <div class="image-caption">16 samples per pixel (16 spp)</div>
    </div>
    
    <h2>Task 3: Transforms</h2>
    
    <p>
        In <code>transforms.cpp</code>, we completed <code>translate</code>, <code>scale</code>, and <code>rotate</code> by producing 3×3 transformation matrices according to the SVG specification. In the screenshot, you can see the cubeman making snow angels.
    </p>
    
    <div class="image-container">
        <img src="./submission-images/screenshot_2-19_0-0-11.png" alt="my_robot.svg">
    </div>
    
    <h2>Task 4: Barycentric Coordinates</h2>
    
    <p>
        Barycentric coordinates \((\alpha, \beta, \gamma)\) allow a point \(P\) inside a triangle with vertices \(V_1, V_2, V_3\) to be represented as 
        \[
           P = \alpha V_1 + \beta V_2 + \gamma V_3,
        \]
        where \(\alpha + \beta + \gamma = 1\). This can be interpreted as expressing every point in the triangle as a weighted average of its vertices. Points outside of the triangle will have one of \((\alpha, \beta, \gamma)\) as a negative number. This also enables per-vertex attributes (e.g., color) to be interpolated smoothly across the triangle's interior, with points closer to the vertices having a similar color to their respective vertex. We updated <code>rasterize_interpolated_color_triangle(...)</code> to compute barycentric coordinates per sample and blend vertex colors accordingly. This produces gradients across triangles rather than a single solid color. This can be seen in the following image, where the vertices of this triangle are red, green, and blue and each pixel is colored as a weighted average of how close it is to each vertex.
    </p>
    
    <div class="image-container">
        <img src="./submission-images/screenshot_2-19_0-19-56.png" alt="blended.svg">
    </div>
    
    <p>A PNG of <strong>svg/basic/test7.svg</strong> showing a smoothly blended color wheel at sample rate 1:</p>
    
    <div class="image-container">
        <img src="./submission-images/screenshot_2-19_0-25-6.png" alt="test7.svg">
    </div>
    
    <h2>Task 5: "Pixel Sampling" for Texture Mapping (15 pts)</h2>
    
    <p>
        Nearest sampling picks the texel closest to the sample coordinate. It's faster but can appear blocky when magnified. Bilinear sampling interpolates among four neighboring texels, producing smoother transitions, especially noticeable when the texture is enlarged. <code>sample_nearest(...)</code> and <code>sample_bilinear(...)</code> in <code>texture.cpp</code> handle wrapping, clamping, and coordinate calculation. <code>sample_nearest(...)</code> simply gets the closest texel, while <code>sample_bilinear(...)</code> gets the four nearest texels and bilinearly interpolates. Shown below are four screenshots demonstrating how nearest vs. bilinear sampling differ at 1 spp and 16 spp:
    </p>
    
    <div class="image-container">
        <img src="./submission-images/screenshot_2-19_19-54-24.png" alt="Nearest @ 1 spp">
        <div class="image-caption">Nearest @ 1 spp</div>
    </div>
    
    <div class="image-container">
        <img src="./submission-images/screenshot_2-19_19-54-26.png" alt="Nearest @ 16 spp">
        <div class="image-caption">Nearest @ 16 spp</div>
    </div>
    
    <div class="image-container">
        <img src="./submission-images/screenshot_2-19_19-54-47.png" alt="Bilinear @ 1 spp">
        <div class="image-caption">Bilinear @ 1 spp</div>
    </div>
    
    <div class="image-container">
        <img src="./submission-images/screenshot_2-19_19-54-49.png" alt="Bilinear @ 16 spp">
        <div class="image-caption">Bilinear @ 16 spp</div>
    </div>
    
    <p>
        Observing the images at higher sample rates reveals how bilinear sampling better suppresses aliasing on texture edges. At 1 sample per pixel, the difference is a lot more apparent, as we are sampling less per pixel rather than over 16 samples per pixel to create a better averaging/smoothing effect. At both 1 sample per pixel and 16 samples per pixel, bilinear sampling results in a smoother image than nearest sampling.
    </p>
    
    <h2>Task 6: "Level Sampling" with Mipmaps (25 pts)</h2>
    
    <p>
        Level sampling is picking the right amount of detail for varying parts of an image by using precomputed "mipmaps" of different resolutions. By analyzing the derivatives of texture coordinates in the screen space, we choose the corresponding mipmap level to utilize the right amount of detail in different parts of the image. This helps avoid minification aliasing, or when a texture is viewed far away or rendered very small.
    </p>
    
    <p>
        <code>get_level(...)</code> determines which mipmap level to use by analyzing the derivatives of texture coordinates in screen space based on the sampling method. <code>L_ZERO</code> always samples the base (highest resolution) level. <code>L_NEAREST</code> chooses the nearest mipmap level based on screen-space derivative magnitude. <code>L_LINEAR</code> blends between two adjacent levels for a smoother transition, which becomes trilinear filtering if combined with bilinear pixel sampling.
    </p>
    
    <p>
        By sampling less per pixel, pixel sampling is best in speed and memory, but is weakest against aliasing.
    </p>
    
    <p>
        Level sampling is slower and requires more memory when generating and searching the different mipmap levels, but is stronger against aliasing, as it dynamically changes the level based on the derivative of the texture coordinates in screen space.
    </p>
    
    <p>
        Sampling more per pixel is the slowest and most memory-intensive, but is the strongest against aliasing, as it uses the average of the samples within each pixel to determine the final pixel color.
    </p>
    
    <p>
        Below are sample comparisons of <strong>L_ZERO</strong> vs. <strong>L_NEAREST</strong> combined with <strong>P_NEAREST</strong> vs. <strong>P_LINEAR</strong>. I chose the Mona Lisa as my image.
    </p>
    
    <div class="image-container">
        <img src="./submission-images/screenshot_2-20_22-52-52.png" alt="L_ZERO + P_NEAREST">
        <div class="image-caption"><code>L_ZERO</code> + <code>P_NEAREST</code></div>
    </div>
    
    <div class="image-container">
        <img src="./submission-images/screenshot_2-20_22-53-4.png" alt="L_ZERO + P_LINEAR">
        <div class="image-caption"><code>L_ZERO</code> + <code>P_LINEAR</code></div>
    </div>
    
    <div class="image-container">
        <img src="./submission-images/screenshot_2-20_22-53-10.png" alt="L_NEAREST + P_NEAREST">
        <div class="image-caption"><code>L_NEAREST</code> + <code>P_NEAREST</code></div>
    </div>
    
    <div class="image-container">
        <img src="./submission-images/screenshot_2-20_22-53-17.png" alt="L_NEAREST + P_LINEAR">
        <div class="image-caption"><code>L_NEAREST</code> + <code>P_LINEAR</code></div>
    </div>
    
    <h2>Generative-AI Tools</h2>
    <p>
        Cursor assisted in writing both the code and the writeup, but both have been extensively created and refined by human input. It did not produce great results, so we went over the results in fine detail for debugging and understand the code we have written.
    </p>
    
    <footer>
        <p>CS 184/284A - Spring 2025 - UC Berkeley</p>
    </footer>
</body>
</html> 